4:"$Sreact.fragment"
5:I[7868,[],""]
6:I[8627,[],""]
8:I[4097,[],"OutletBoundary"]
a:I[4097,[],"MetadataBoundary"]
c:I[4097,[],"ViewportBoundary"]
e:I[4154,[],""]
1:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
2:HL["/_next/static/css/42c0b5c9bca926ad.css","style"]
3:HL["/_next/static/css/92d26e0b7acac284.css","style"]
0:{"P":null,"b":"ozTPq29MNYSY8iBIOtNqu","p":"","c":["","posts","vision-agent"],"i":false,"f":[[["",{"children":["posts",{"children":[["slug","vision-agent","d"],{"children":["__PAGE__?{\"slug\":\"vision-agent\"}",{}]}]}]},"$undefined","$undefined",true],["",["$","$4","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/42c0b5c9bca926ad.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","link",null,{"rel":"apple-touch-icon","sizes":"180x180","href":"/favicon/apple-touch-icon.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"32x32","href":"/favicon/favicon-32x32.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"16x16","href":"/favicon/favicon-16x16.png"}],["$","link",null,{"rel":"manifest","href":"/favicon/site.webmanifest"}],["$","link",null,{"rel":"mask-icon","href":"/favicon/safari-pinned-tab.svg","color":"#000000"}],["$","link",null,{"rel":"shortcut icon","href":"/favicon/favicon.ico"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"msapplication-config","content":"/favicon/browserconfig.xml"}],["$","meta",null,{"name":"theme-color","content":"#000"}],["$","link",null,{"rel":"alternate","type":"application/rss+xml","href":"/feed.xml"}]]}],["$","body",null,{"className":"__className_d65c78 dark:bg-slate-900 dark:text-slate-400","children":["$","div",null,{"className":"min-h-screen","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]}]]}]]}],{"children":["posts",["$","$4","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]]}],{"children":[["slug","vision-agent","d"],["$","$4","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]]}],{"children":["__PAGE__",["$","$4","c",{"children":["$L7",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/92d26e0b7acac284.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L8",null,{"children":"$L9"}]]}],{},null]},null]},null]},null],["$","$4","h",{"children":[null,["$","$4","7ucSHqbXDJqcjCaKCx-4H",{"children":[["$","$La",null,{"children":"$Lb"}],["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust"}]]}]]}]]],"m":"$undefined","G":["$e","$undefined"],"s":false,"S":true}
f:I[6410,["410","static/chunks/410-48b8dbe3f3a5803f.js","333","static/chunks/app/posts/%5Bslug%5D/page-ae76bbdb04cb5ccc.js"],""]
10:T4227,<p>Vision Language Models (VLMs) such as GPT-4o and Claude-3.5 have done well and continue to improve at textual tasks but they still struggle with visual tasks. For example, let’s ask these VLMs to count the number of missing soda cans in this image:</p>
<p><img src="/assets/blog/vision-agent/soda_cans.png" alt="soda cans"></p>
<h2>The Soda Can Puzzle Failure</h2>
<p><strong>Claude-3.5 (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“Looking at the image, I can see cans of Coca-Cola arranged in what appears to be a 4x7 grid pattern (28 total spaces). There are currently 26 cans visible in the image, with 2 empty spaces in the middle of the arrangement. Therefore, there are 2 cans missing from the full set.”</p>
</blockquote>
<p><strong>GPT-4o (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“To determine how many soda cans are missing, we can count the number of slots in the box and compare it to the number of cans present.</p>
<p>Observations:</p>
<p>The soda box appears to have a 6x5 layout (6 rows and 5 columns), which would hold a total of  6 \times 5 = 30  cans if full.
By counting the cans present, I see 28 cans.</p>
<p>Calculation:</p>
<p>The total number of missing cans is: 30 - 28 = 2</p>
<p>Conclusion:</p>
<p>There are 2 soda cans missing from the box.”</p>
</blockquote>
<p><strong>Gemini-2.0-Flash-Exp (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“The image shows a box of soda cans. There are 24 cans in total. However, there are 2 missing.”</p>
</blockquote>
<p><strong>GPT-o1 (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“Answer: 12 cans are missing.</p>
<p>Explanation:
Based on the arrangement shown, it appears the soda cans were originally packed in a rectangular grid. A common configuration for large packs of soda cans (such as a 30-pack) is a 6-by-5 grid, totaling 30 cans.
If we assume the original arrangement was a full 30-can pack (6 cans across by 5 rows deep):
The outer "border" layer of such a rectangle consists of:</p>
<ul>
<li>Top row: 6 cans</li>
<li>Bottom row: 6 cans</li>
<li>Left side (excluding the already counted corners): 3 cans</li>
<li>Right side (likewise): 3 cans</li>
</ul>
<p>Adding these up:</p>
<ul>
<li>Top: 6</li>
<li>Bottom: 6</li>
<li>Left (without corners): 3</li>
<li>Right (without corners): 3</li>
<li>Total outer ring = 6 + 6 + 3 + 3 = 18 cans</li>
</ul>
<p>If only the outer ring of cans remains visible and the entire inner portion is empty, then:</p>
<ul>
<li>Total originally: 30 cans</li>
<li>Currently visible (outer ring): 18 cans</li>
<li>Missing cans = 30 - 18 = 12</li>
</ul>
<p>Thus, there are 12 missing cans.”</p>
</blockquote>
<p>The soda cans are clearly a 5x7 grid with 4 missing cans. Both models not only get the answer wrong, but they also cannot identify the grid pattern. Furthermore, if you ask them to draw a box around each missing can, they will fail.</p>
<h2>Overview</h2>
<p>In this blog post we introduce <a href="https://va.landing.ai/agent">VisionAgent</a>, an agentic AI framework that can handle difficult visual tasks that require reasoning. In the first part we will introduce the overall agentic framework for VisionAgent and how it modularizes tool choice to pick the best visual tool for a task. In the second part we will show how we can update the model to use design patterns to solve particularly complex visual problems.</p>
<h2>An Agentic Approach to Visual Reasoning</h2>
<p>To get started, let’s first consider how a human would solve the missing soda can problem. When I solve this task I first locate an empty-looking region in the soda can grid. I then locate cans near the empty region; in this case above and below the regions. To count the number of missing cans I form a sort of grid, so I can see there are 2 cans above the empty spot and therefore the empty spot must consist of 2 missing cans.</p>
<p><img src="/assets/blog/vision-agent/soda_cans.png" alt="soda cans small"></p>
<p>Now VLMs are not good at localization, so they likely cannot find an empty-looking region like I did. Because they aren’t good at localization, they also cannot find the locations of the soda cans, but there are many tools that would be able to accomplish this task, such as CountGD. So the first task of the agent could be to find a tool that can locate soda cans.</p>
<p><img src="/assets/blog/vision-agent/soda_can_dets.png" alt="soda cans dets"></p>
<p>Given that the agent now has the locations of all the soda cans, it can use this information to construct a grid pattern over all the soda cans. Finally, given the soda can detections, and the full grid, it can simply check if each cell contains a soda can detection; if not, it can be counted as a missing can.</p>
<table>
<thead>
<tr>
<th align="center">grid</th>
<th align="center">dets</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/grid_image.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/all_dets.jpg" alt=""></td>
</tr>
</tbody>
</table>
<p>From these steps you can start to see an <strong>agentic framework</strong> forming. We can break this down this into 3 main components:</p>
<ol>
<li><strong>Agentic Tool Selection and Subtasks</strong> - The overall problem gets divided into solvable subtasks. For instance, in our running example, the first subtask was formulated to detect soda cans, second subtask for constructing the grid pattern and the final subtask for matching detected soda cans to grid cells to find missing soda cans. For each subtask, optimal tool selection has to be performed agentically.</li>
<li><strong>Composing Subtasks</strong> - We had to compose the subtasks such that the outputs of the previous subtask(s) can be used to solve the new subtasks. For example, to construct the grid we needed the soda can detections from the previous subtask, and to find the missing soda cans we needed the grid and the soda can detections from the previous 2 subtasks.</li>
<li><strong>Visual Design Patterns</strong> - Finally, since VLMs struggle at tasks that require visual reasoning, we need to give the agent a hint by telling it about a design pattern it can use to solve this problem. For example, suggesting that it construct a grid and use the grid to detect missing cans. We will cover this in detail in the next blog post.</li>
</ol>
<h2>Implementing a Visual Reasoning Agent</h2>
<p>To implement this visual reasoning agent, we need to incorporate the three components mentioned above. Before we do that, however, we need a data structure that can handle both inputs to and outputs from these component–allowing the agent to observe and utilize them seamlessly. One way to achieve this is by borrowing the concept of an event stream from <a href="https://arxiv.org/abs/2407.16741">“Open Hands”</a>.</p>
<h3>Event Stream Data Structure</h3>
<p>An <strong>event stream</strong> is a structure that:</p>
<ul>
<li><strong>Stores</strong> user messages and agent messages</li>
<li><strong>Dispatches</strong> actions (like calls to external tools)</li>
<li><strong>Captures</strong> the results or observations of those actions</li>
</ul>
<p><img src="/assets/blog/vision-agent/event_stream.png" alt="event stream"></p>
<p>In the diagram mentioned above, note that the box on the left includes <strong>USER</strong>, <strong>AGENT</strong>, and <strong>OBSERVATION</strong>. Each <strong>AGENT</strong> message can contain:</p>
<ul>
<li><code>&#x3C;thinking></code> tags for deciding what to do next,</li>
<li><code>&#x3C;python></code> tags, reflecting the approach from the paper <a href="https://arxiv.org/abs/2402.01030">“Executable Code Actions Elicit Better LLM Agents"</a>. Giving the agent the ability to execute code (rather than just returning text/JSON) has been shown to improve performance. We let it run these Python snippets in a Jupyter notebook–like environment. We call this main agent the planner agent.</li>
</ul>
<h3>Agentic Tool Selection and Subtasks</h3>
<p>Having established the <strong>event stream</strong> and basic workflow, we can now discuss how to implement the three core components of our Agentic Framework. Referring back to the soda-can example:</p>
<ol>
<li><strong>Tool Selection for Detecting Soda Cans</strong> The first subtask is detecting the soda cans themselves. We delegate this to a specialized subagent—the tool choice agent—which has access to multiple object-detection tools. Its job is to run these tools on the image and pick the one that performs best for our current task. Since we use code as action, we might represent this by a function <code>get_tool_for_task(image, description)</code> that returns documentation (or references) for the chosen tool.</li>
</ol>
<p><img src="/assets/blog/vision-agent/tool_choice.png" alt="tool choice"></p>
<p>The agent uses ‘get_tool_for_task’ to compare multiple detectors (OwlV2 and CountGD), automatically picking the one best suited for soda-can detection.</p>
<ol start="2">
<li><strong>Why Tool Choice Matters</strong> Selecting the correct tool is challenging, so we handle it in a dedicated tool choice agent. First, the agent interprets the prompt to determine what we’re detecting. It retrieves relevant tools and their documentation, writes code to test each tool on the image, and prints the outputs. A judge agent then compares these outputs to its own approximate prediction.</li>
</ol>
<p><strong>Note on the Judge Agent:</strong>
The judge agent doesn’t need to provide an exact count—it just needs to identify which detection result is most plausible based on approximate reasoning. For instance, if the judge guesses 28 cans but sees one tool reporting 5 and another reporting 27, it can rule out 5 as too far off. This allows the system to pick the best available tool without requiring the judge to be perfect. Replacing the judge agent with a human is also an option—sometimes a simple visual check is quicker than labeling everything in detail.</p>
<h3>Composing Subtasks</h3>
<p>Once we have the right tool, composing subtasks is straightforward thanks to our “actions as code” approach. The planner agent uses Python to pass outputs from one subtask as inputs to the next. For example:</p>
<pre><code class="language-python"># Step 1: Load the image and pick the best tool for soda-can detection
image = load_image("image.png")
get_tool_for_task("Detect all soda cans in the image", [image])

# Step 2: Use the chosen tool (e.g. CountGD) to perform detection
detections = countgd("soda can", image)

# Step 3: Create a grid pattern based on the detections
grid = create_grid_pattern(detections)
</code></pre>
<p>This modular setup makes it easy to debug each stage. If the detected boxes look incorrect, the planner can try a different confidence threshold or switch to another tool. Because each action is executable Python code, the process of re-running or re-parameterizing is smooth and transparent.</p>
<h2>Visual Design Patterns</h2>
<p>The final component is <strong>visual design patterns</strong>. We implement this by giving the planner agent a special action called “suggestions,” which serves as a lookup for known design patterns relevant to our problem. The planner agent submits a prompt and image, receiving a 'visual design pattern' suggestion if a suitable one is available.</p>
<h3>What are Visual Design Patterns?</h3>
<p>Before we dive into the details of this component, it’s important to set some context. If you’ve ever worked on Visual AI projects, you’ll probably know that there are certain visual patterns in images and problems that you can identify and use to draw from a pool of techniques that can be used to solve those problems. Take for example the visual grid pattern and the corresponding design pattern of using an object detection model to construct a grid and identify missing objects. This same pattern actually pops up in many different situations:</p>
<table>
<thead>
<tr>
<th align="center">egg carton</th>
<th align="center">parking lot</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/egg_carton.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/parking_lot.png" alt=""></td>
</tr>
<tr>
<td align="center"><strong>housing development</strong></td>
<td align="center"><strong>CPU pins</strong></td>
</tr>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/houses.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/pins.jpg" alt=""></td>
</tr>
</tbody>
</table>
<p>So you can see how having a design pattern to solve one type of problem can be applied to many use cases.</p>
<h3>Visual Design Pattern Implementation</h3>
<p>To build the visual design pattern component we have labeled different images and user questions and their corresponding design patterns used to solve them. The component works by taking in the target image and a query, generated from the planner such as “How can I count the number of missing soda cans in this image?” The component then decides if the image and query meet one of the visual patterns we have labeled. If so, it retrieves the design pattern solution, which is a pseudocode solution on how to solve a problem containing that particular visual pattern. This is then returned back to the planner discussed in the previous blog post.</p>
<p><img src="/assets/blog/vision-agent/design_patterns.png" alt="visual design patterns"></p>
<p>For example:</p>
<ul>
<li>A user prompt might say, “Identify how many cans are missing in this regular grid.”</li>
<li>The suggestions action checks if any known pattern (like a grid-based arrangement) applies.</li>
<li>If it matches, the action returns a design pattern outline: “Use the detected bounding boxes to infer grid cells, then compare each cell to see which are empty.”</li>
<li>This approach helps the planner agent tackle more complex tasks by following structured strategies—especially when straightforward detection alone isn’t enough.</li>
</ul>
<h2>Putting it All Together</h2>
<p>Now that we have covered all 3 components: agentic tool selection and subtasks, composing subtasks and visual design patterns, how do they all work together? We give the planner agent a budget of 10 steps to make when trying to solve a problem, and we also give the planner the ability to use libraries like Matplotlib to plot and visualize results. This allows the planner to go down a path, decide it doesn’t like the results, and then backtrack and find a new path. For example, the planner could use the plan we have described in this blog to find soda cans, but after visualizing the results decide that CountGD did not do a good enough job. It can then backtrack and find a tool to filter the CountGD results such as SigLIP, and then start a new plan where it crops the CountGD objects and filters them with SigLIP to improve performance.</p>
<p><img src="/assets/blog/vision-agent/back_tracking.png" alt="back tracking"></p>
<p>With these components and agentic workflow in place we can now better reason about vision tasks.</p>
<p>The approach we have taken with VisionAgent can be seen as program synthesis, a topic discussed in Francois Chollet’s <a href="https://arxiv.org/abs/1911.01547">On the Measure of Intelligence</a> paper, where an intelligent system generates a “program” that can be used to solve a task.</p>
<p><img src="/assets/blog/vision-agent/measure_of_intelligence.png" alt="measure of intelligence"></p>
<p>Here, VisionAgent is the “Intelligent system” and the program it generates for a given task is the “Skill program”. Our tasks are different in the sense that they are vision problems so they do not produce a response or score. However, you can also think of our tasks as the human evaluating the VisionAgent solution, who can provide a response to VisionAgent if it does not perform the task correctly or evaluate it with a score</p>
<h2>Conclusion</h2>
<p>In this blog post we covered how to use visual design patterns to further enhance our agents visual reasoning skills. We can do all sorts of cool things with this in addition to just finding missing soda cans, such as tracking a queue ball in a pool game, or counting the number of smaller screws on a table. You can checkout the <a href="https://github.com/landing-ai/vision-agent">source code</a> <a href="https://github.com/landing-ai/vision-agent">https://github.com/landing-ai/vision-agent</a> if you want to play around with it for yourself. Here's a few more examples of what VisionAgent can do:</p>
<p><strong>Prompt:</strong> Count the number of missing soda cans and draw a bounding box around them.
<img src="/assets/blog/vision-agent/missing_cans_viz.png" alt="missing cans"></p>
<p><strong>Prompt:</strong> Count the smaller screws in the image.
<img src="/assets/blog/vision-agent/screws_viz.png" alt="screws"></p>
7:["$","main",null,{"children":["$","div",null,{"className":"container mx-auto px-5","children":[["$","h2",null,{"className":"text-2xl md:text-4xl font-bold tracking-tight md:tracking-tighter leading-tight mb-20 mt-8 flex items-center","children":[["$","$Lf",null,{"href":"/","className":"hover:underline","children":"Home"}],"."]}],["$","article",null,{"className":"mb-32","children":[["$","h1",null,{"className":"text-5xl md:text-7xl lg:text-8xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center md:text-left","children":"VisionAgent: An Agentic Approach for Complex Visual Reasoning"}],["$","div",null,{"className":"max-w-4xl mx-auto","children":["$","div",null,{"className":"markdown-styles_markdown__ORfR8","dangerouslySetInnerHTML":{"__html":"$10"}}]}]]}]]}]}]
d:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"VisionAgent: An Agentic Approach for Complex Visual Reasoning | Dillon Laird"}],["$","meta","2",{"name":"description","content":"Dillon Laird's Personal Website"}],["$","meta","3",{"property":"og:title","content":"VisionAgent: An Agentic Approach for Complex Visual Reasoning | Dillon Laird"}],["$","meta","4",{"property":"og:description","content":"Dillon Laird's Personal Website"}],["$","meta","5",{"property":"og:image","content":"http://localhost:3000/assets/blog/vision-agent/logo_dark.svg"}],["$","meta","6",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","7",{"name":"twitter:title","content":"VisionAgent: An Agentic Approach for Complex Visual Reasoning | Dillon Laird"}],["$","meta","8",{"name":"twitter:description","content":"Dillon Laird's Personal Website"}],["$","meta","9",{"name":"twitter:image","content":"http://localhost:3000/assets/blog/vision-agent/logo_dark.svg"}]]
9:null
