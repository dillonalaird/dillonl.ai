<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/42c0b5c9bca926ad.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/92d26e0b7acac284.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-39e94b047d8ca4ed.js"/><script src="/_next/static/chunks/e5a3b8ce-a19909d46d3c4526.js" async=""></script><script src="/_next/static/chunks/911-ba2ba010a4f0f8a6.js" async=""></script><script src="/_next/static/chunks/main-app-eae8022f01765963.js" async=""></script><script src="/_next/static/chunks/410-48b8dbe3f3a5803f.js" async=""></script><script src="/_next/static/chunks/app/posts/%5Bslug%5D/page-ae76bbdb04cb5ccc.js" async=""></script><meta name="next-size-adjust"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><title>VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird</title><meta name="description" content="Dillon Laird&#x27;s Personal Website"/><meta property="og:title" content="VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird"/><meta property="og:description" content="Dillon Laird&#x27;s Personal Website"/><meta property="og:image" content="http://localhost:3000/assets/blog/vision-agent/logo_dark.svg"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird"/><meta name="twitter:description" content="Dillon Laird&#x27;s Personal Website"/><meta name="twitter:image" content="http://localhost:3000/assets/blog/vision-agent/logo_dark.svg"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78 dark:bg-slate-900 dark:text-slate-400"><div class="min-h-screen"><main><div class="container mx-auto px-5"><h2 class="text-2xl md:text-4xl font-bold tracking-tight md:tracking-tighter leading-tight mb-20 mt-8 flex items-center"><a class="hover:underline" href="/">Home</a>.</h2><article class="mb-32"><h1 class="text-5xl md:text-7xl lg:text-8xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center md:text-left">VisionAgent: An Agentic Approach for Complex Reasoning</h1><div class="max-w-4xl mx-auto"><div class="markdown-styles_markdown__ORfR8"><p>Vision Language Models (VLMs) such as GPT-4o and Claude-3.5 have done well and continue to improve at textual tasks but they still struggle with visual tasks. For example, let’s ask these VLMs to count the number of missing soda cans in this image:</p>
<p><img src="/assets/blog/vision-agent/soda_cans.png" alt="soda cans"></p>
<h2>The Soda Can Puzzle Failure</h2>
<p><strong>Claude-3.5 (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“Looking at the image, I can see cans of Coca-Cola arranged in what appears to be a 4x7 grid pattern (28 total spaces). There are currently 26 cans visible in the image, with 2 empty spaces in the middle of the arrangement. Therefore, there are 2 cans missing from the full set.”</p>
</blockquote>
<p><strong>GPT-4o (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“To determine how many soda cans are missing, we can count the number of slots in the box and compare it to the number of cans present.</p>
<p>Observations:</p>
<p>The soda box appears to have a 6x5 layout (6 rows and 5 columns), which would hold a total of  6 \times 5 = 30  cans if full.
By counting the cans present, I see 28 cans.</p>
<p>Calculation:</p>
<p>The total number of missing cans is: 30 - 28 = 2</p>
<p>Conclusion:</p>
<p>There are 2 soda cans missing from the box.”</p>
</blockquote>
<p><strong>Gemini-2.0-Flash-Exp (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“The image shows a box of soda cans. There are 24 cans in total. However, there are 2 missing.”</p>
</blockquote>
<p><strong>GPT-o1 (tested on 1/1/2025):</strong></p>
<blockquote>
<p>“Answer: 12 cans are missing.</p>
<p>Explanation:
Based on the arrangement shown, it appears the soda cans were originally packed in a rectangular grid. A common configuration for large packs of soda cans (such as a 30-pack) is a 6-by-5 grid, totaling 30 cans.
If we assume the original arrangement was a full 30-can pack (6 cans across by 5 rows deep):
The outer "border" layer of such a rectangle consists of:</p>
<ul>
<li>Top row: 6 cans</li>
<li>Bottom row: 6 cans</li>
<li>Left side (excluding the already counted corners): 3 cans</li>
<li>Right side (likewise): 3 cans</li>
</ul>
<p>Adding these up:</p>
<ul>
<li>Top: 6</li>
<li>Bottom: 6</li>
<li>Left (without corners): 3</li>
<li>Right (without corners): 3</li>
<li>Total outer ring = 6 + 6 + 3 + 3 = 18 cans</li>
</ul>
<p>If only the outer ring of cans remains visible and the entire inner portion is empty, then:</p>
<ul>
<li>Total originally: 30 cans</li>
<li>Currently visible (outer ring): 18 cans</li>
<li>Missing cans = 30 - 18 = 12</li>
</ul>
<p>Thus, there are 12 missing cans.”</p>
</blockquote>
<p>The soda cans are clearly a 5x7 grid with 4 missing cans. Both models not only get the answer wrong, but they also cannot identify the grid pattern. Furthermore, if you ask them to draw a box around each missing can, they will fail.</p>
<h2>Overview</h2>
<p>In this blog post we introduce <a href="https://va.landing.ai/agent">VisionAgent</a>, an agentic AI framework that can handle difficult visual tasks that require reasoning. In the first part we will introduce the overall agentic framework for VisionAgent and how it modularizes tool choice to pick the best visual tool for a task. In the second part we will show how we can update the model to use design patterns to solve particularly complex visual problems.</p>
<h2>An Agentic Approach to Visual Reasoning</h2>
<p>To get started, let’s first consider how a human would solve the missing soda can problem. When I solve this task I first locate an empty-looking region in the soda can grid. I then locate cans near the empty region; in this case above and below the regions. To count the number of missing cans I form a sort of grid, so I can see there are 2 cans above the empty spot and therefore the empty spot must consist of 2 missing cans.</p>
<p><img src="/assets/blog/vision-agent/soda_cans.png" alt="soda cans small"></p>
<p>Now VLMs are not good at localization, so they likely cannot find an empty-looking region like I did. Because they aren’t good at localization, they also cannot find the locations of the soda cans, but there are many tools that would be able to accomplish this task, such as CountGD. So the first task of the agent could be to find a tool that can locate soda cans.</p>
<p><img src="/assets/blog/vision-agent/soda_can_dets.png" alt="soda cans dets"></p>
<p>Given that the agent now has the locations of all the soda cans, it can use this information to construct a grid pattern over all the soda cans. Finally, given the soda can detections, and the full grid, it can simply check if each cell contains a soda can detection; if not, it can be counted as a missing can.</p>
<table>
<thead>
<tr>
<th align="center">grid</th>
<th align="center">dets</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/grid_image.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/all_dets.jpg" alt=""></td>
</tr>
</tbody>
</table>
<p>From these steps you can start to see an <strong>agentic framework</strong> forming. We can break this down this into 3 main components:</p>
<ol>
<li><strong>Agentic Tool Selection and Subtasks</strong> - The overall problem gets divided into solvable subtasks. For instance, in our running example, the first subtask was formulated to detect soda cans, second subtask for constructing the grid pattern and the final subtask for matching detected soda cans to grid cells to find missing soda cans. For each subtask, optimal tool selection has to be performed agentically.</li>
<li><strong>Composing Subtasks</strong> - We had to compose the subtasks such that the outputs of the previous subtask(s) can be used to solve the new subtasks. For example, to construct the grid we needed the soda can detections from the previous subtask, and to find the missing soda cans we needed the grid and the soda can detections from the previous 2 subtasks.</li>
<li><strong>Visual Design Patterns</strong> - Finally, since VLMs struggle at tasks that require visual reasoning, we need to give the agent a hint by telling it about a design pattern it can use to solve this problem. For example, suggesting that it construct a grid and use the grid to detect missing cans. We will cover this in detail in the next blog post.</li>
</ol>
<h2>Implementing a Visual Reasoning Agent</h2>
<p>To implement this visual reasoning agent, we need to incorporate the three components mentioned above. Before we do that, however, we need a data structure that can handle both inputs to and outputs from these component–allowing the agent to observe and utilize them seamlessly. One way to achieve this is by borrowing the concept of an event stream from <a href="https://arxiv.org/abs/2407.16741">“Open Hands”</a>.</p>
<h3>Event Stream Data Structure</h3>
<p>An <strong>event stream</strong> is a structure that:</p>
<ul>
<li><strong>Stores</strong> user messages and agent messages</li>
<li><strong>Dispatches</strong> actions (like calls to external tools)</li>
<li><strong>Captures</strong> the results or observations of those actions</li>
</ul>
<p><img src="/assets/blog/vision-agent/event_stream.png" alt="event stream"></p>
<p>In the diagram mentioned above, note that the box on the left includes <strong>USER</strong>, <strong>AGENT</strong>, and <strong>OBSERVATION</strong>. Each <strong>AGENT</strong> message can contain:</p>
<ul>
<li><code>&#x3C;thinking></code> tags for deciding what to do next,</li>
<li><code>&#x3C;python></code> tags, reflecting the approach from the paper <a href="https://arxiv.org/abs/2402.01030">“Executable Code Actions Elicit Better LLM Agents"</a>. Giving the agent the ability to execute code (rather than just returning text/JSON) has been shown to improve performance. We let it run these Python snippets in a Jupyter notebook–like environment. We call this main agent the planner agent.</li>
</ul>
<h3>Agentic Tool Selection and Subtasks</h3>
<p>Having established the <strong>event stream</strong> and basic workflow, we can now discuss how to implement the three core components of our Agentic Framework. Referring back to the soda-can example:</p>
<ol>
<li><strong>Tool Selection for Detecting Soda Cans</strong> The first subtask is detecting the soda cans themselves. We delegate this to a specialized subagent—the tool choice agent—which has access to multiple object-detection tools. Its job is to run these tools on the image and pick the one that performs best for our current task. Since we use code as action, we might represent this by a function <code>get_tool_for_task(image, description)</code> that returns documentation (or references) for the chosen tool.</li>
</ol>
<p><img src="/assets/blog/vision-agent/tool_choice.png" alt="tool choice"></p>
<p>The agent uses ‘get_tool_for_task’ to compare multiple detectors (OwlV2 and CountGD), automatically picking the one best suited for soda-can detection.</p>
<ol start="2">
<li><strong>Why Tool Choice Matters</strong> Selecting the correct tool is challenging, so we handle it in a dedicated tool choice agent. First, the agent interprets the prompt to determine what we’re detecting. It retrieves relevant tools and their documentation, writes code to test each tool on the image, and prints the outputs. A judge agent then compares these outputs to its own approximate prediction.</li>
</ol>
<p><strong>Note on the Judge Agent:</strong>
The judge agent doesn’t need to provide an exact count—it just needs to identify which detection result is most plausible based on approximate reasoning. For instance, if the judge guesses 28 cans but sees one tool reporting 5 and another reporting 27, it can rule out 5 as too far off. This allows the system to pick the best available tool without requiring the judge to be perfect. Replacing the judge agent with a human is also an option—sometimes a simple visual check is quicker than labeling everything in detail.</p>
<h3>Composing Subtasks</h3>
<p>Once we have the right tool, composing subtasks is straightforward thanks to our “actions as code” approach. The planner agent uses Python to pass outputs from one subtask as inputs to the next. For example:</p>
<pre><code class="language-python"># Step 1: Load the image and pick the best tool for soda-can detection
image = load_image("image.png")
get_tool_for_task("Detect all soda cans in the image", [image])

# Step 2: Use the chosen tool (e.g. CountGD) to perform detection
detections = countgd("soda can", image)

# Step 3: Create a grid pattern based on the detections
grid = create_grid_pattern(detections)
</code></pre>
<p>This modular setup makes it easy to debug each stage. If the detected boxes look incorrect, the planner can try a different confidence threshold or switch to another tool. Because each action is executable Python code, the process of re-running or re-parameterizing is smooth and transparent.</p>
<h2>Visual Design Patterns</h2>
<p>The final component is <strong>visual design patterns</strong>. We implement this by giving the planner agent a special action called “suggestions,” which serves as a lookup for known design patterns relevant to our problem. The planner agent submits a prompt and image, receiving a 'visual design pattern' suggestion if a suitable one is available.</p>
<h3>What are Visual Design Patterns?</h3>
<p>Before we dive into the details of this component, it’s important to set some context. If you’ve ever worked on Visual AI projects, you’ll probably know that there are certain visual patterns in images and problems that you can identify and use to draw from a pool of techniques that can be used to solve those problems. Take for example the visual grid pattern and the corresponding design pattern of using an object detection model to construct a grid and identify missing objects. This same pattern actually pops up in many different situations:</p>
<table>
<thead>
<tr>
<th align="center">egg carton</th>
<th align="center">parking lot</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/egg_carton.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/parking_lot.png" alt=""></td>
</tr>
<tr>
<td align="center"><strong>housing development</strong></td>
<td align="center"><strong>CPU pins</strong></td>
</tr>
<tr>
<td align="center"><img src="/assets/blog/vision-agent/houses.png" alt=""></td>
<td align="center"><img src="/assets/blog/vision-agent/pins.jpg" alt=""></td>
</tr>
</tbody>
</table>
<p>So you can see how having a design pattern to solve one type of problem can be applied to many use cases.</p>
<h3>Visual Design Pattern Implementation</h3>
<p>To build the visual design pattern component we have labeled different images and user questions and their corresponding design patterns used to solve them. The component works by taking in the target image and a query, generated from the planner such as “How can I count the number of missing soda cans in this image?” The component then decides if the image and query meet one of the visual patterns we have labeled. If so, it retrieves the design pattern solution, which is a pseudocode solution on how to solve a problem containing that particular visual pattern. This is then returned back to the planner discussed in the previous blog post.</p>
<p><img src="/assets/blog/vision-agent/design_patterns.png" alt="visual design patterns"></p>
<p>For example:</p>
<ul>
<li>A user prompt might say, “Identify how many cans are missing in this regular grid.”</li>
<li>The suggestions action checks if any known pattern (like a grid-based arrangement) applies.</li>
<li>If it matches, the action returns a design pattern outline: “Use the detected bounding boxes to infer grid cells, then compare each cell to see which are empty.”</li>
<li>This approach helps the planner agent tackle more complex tasks by following structured strategies—especially when straightforward detection alone isn’t enough.</li>
</ul>
<h2>Putting it All Together</h2>
<p>Now that we have covered all 3 components: agentic tool selection and subtasks, composing subtasks and visual design patterns, how do they all work together? We give the planner agent a budget of 10 steps to make when trying to solve a problem, and we also give the planner the ability to use libraries like Matplotlib to plot and visualize results. This allows the planner to go down a path, decide it doesn’t like the results, and then backtrack and find a new path. For example, the planner could use the plan we have described in this blog to find soda cans, but after visualizing the results decide that CountGD did not do a good enough job. It can then backtrack and find a tool to filter the CountGD results such as SigLIP, and then start a new plan where it crops the CountGD objects and filters them with SigLIP to improve performance.</p>
<p><img src="/assets/blog/vision-agent/back_tracking.png" alt="back tracking"></p>
<p>With these components and agentic workflow in place we can now better reason about vision tasks.</p>
<p>The approach we have taken with VisionAgent can be seen as program synthesis, a topic discussed in Francois Chollet’s <a href="https://arxiv.org/abs/1911.01547">On the Measure of Intelligence</a> paper, where an intelligent system generates a “program” that can be used to solve a task.</p>
<p><img src="/assets/blog/vision-agent/measure_of_intelligence.png" alt="measure of intelligence"></p>
<p>Here, VisionAgent is the “Intelligent system” and the program it generates for a given task is the “Skill program”. Our tasks are different in the sense that they are vision problems so they do not produce a response or score. However, you can also think of our tasks as the human evaluating the VisionAgent solution, who can provide a response to VisionAgent if it does not perform the task correctly or evaluate it with a score</p>
<h2>Conclusion</h2>
<p>In this blog post we covered how to use visual design patterns to further enhance our agents visual reasoning skills. We can do all sorts of cool things with this in addition to just finding missing soda cans, such as tracking a queue ball in a pool game, or counting the number of smaller screws on a table. Check out the following prompts and the results from VisionAgent.</p>
<p><strong>Prompt:</strong> Count the number of missing soda cans and draw a bounding box around them.
<img src="/assets/blog/vision-agent/missing_cans_viz.png" alt="missing cans"></p>
<p><strong>Prompt:</strong> Count the smaller screws in the image.
<img src="/assets/blog/vision-agent/screws_viz.png" alt="screws"></p>
</div></div></article></div></main></div><script src="/_next/static/chunks/webpack-39e94b047d8ca4ed.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"4:\"$Sreact.fragment\"\n5:I[7868,[],\"\"]\n6:I[8627,[],\"\"]\n8:I[4097,[],\"OutletBoundary\"]\na:I[4097,[],\"MetadataBoundary\"]\nc:I[4097,[],\"ViewportBoundary\"]\ne:I[4154,[],\"\"]\n1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/42c0b5c9bca926ad.css\",\"style\"]\n3:HL[\"/_next/static/css/92d26e0b7acac284.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"tQDrf_5kn6gnuNx2sYZyR\",\"p\":\"\",\"c\":[\"\",\"posts\",\"vision-agent\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"vision-agent\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"vision-agent\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$4\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/42c0b5c9bca926ad.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"180x180\",\"href\":\"/favicon/apple-touch-icon.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/favicon/favicon-32x32.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/favicon/favicon-16x16.png\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/favicon/site.webmanifest\"}],[\"$\",\"link\",null,{\"rel\":\"mask-icon\",\"href\":\"/favicon/safari-pinned-tab.svg\",\"color\":\"#000000\"}],[\"$\",\"link\",null,{\"rel\":\"shortcut icon\",\"href\":\"/favicon/favicon.ico\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-config\",\"content\":\"/favicon/browserconfig.xml\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}]]}],[\"$\",\"body\",null,{\"className\":\"__className_d65c78 dark:bg-slate-900 dark:text-slate-400\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]]}]]}],{\"children\":[\"posts\",[\"$\",\"$4\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"vision-agent\",\"d\"],[\"$\",\"$4\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$4\",\"c\",{\"children\":[\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/92d26e0b7acac284.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}]]}],{},null]},null]},null]},null],[\"$\",\"$4\",\"h\",{\"children\":[null,[\"$\",\"$4\",\"zSmpOz7xAO3fFnFYtGCYr\",{\"children\":[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\"}]]}]]}]]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[6410,[\"410\",\"static/chunks/410-48b8dbe3f3a5803f.js\",\"333\",\"static/chunks/app/posts/%5Bslug%5D/page-ae76bbdb04cb5ccc.js\"],\"\"]\n10:T4143,"])</script><script>self.__next_f.push([1,"\u003cp\u003eVision Language Models (VLMs) such as GPT-4o and Claude-3.5 have done well and continue to improve at textual tasks but they still struggle with visual tasks. For example, let’s ask these VLMs to count the number of missing soda cans in this image:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/soda_cans.png\" alt=\"soda cans\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Soda Can Puzzle Failure\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eClaude-3.5 (tested on 1/1/2025):\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Looking at the image, I can see cans of Coca-Cola arranged in what appears to be a 4x7 grid pattern (28 total spaces). There are currently 26 cans visible in the image, with 2 empty spaces in the middle of the arrangement. Therefore, there are 2 cans missing from the full set.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGPT-4o (tested on 1/1/2025):\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“To determine how many soda cans are missing, we can count the number of slots in the box and compare it to the number of cans present.\u003c/p\u003e\n\u003cp\u003eObservations:\u003c/p\u003e\n\u003cp\u003eThe soda box appears to have a 6x5 layout (6 rows and 5 columns), which would hold a total of  6 \\times 5 = 30  cans if full.\nBy counting the cans present, I see 28 cans.\u003c/p\u003e\n\u003cp\u003eCalculation:\u003c/p\u003e\n\u003cp\u003eThe total number of missing cans is: 30 - 28 = 2\u003c/p\u003e\n\u003cp\u003eConclusion:\u003c/p\u003e\n\u003cp\u003eThere are 2 soda cans missing from the box.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGemini-2.0-Flash-Exp (tested on 1/1/2025):\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“The image shows a box of soda cans. There are 24 cans in total. However, there are 2 missing.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGPT-o1 (tested on 1/1/2025):\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Answer: 12 cans are missing.\u003c/p\u003e\n\u003cp\u003eExplanation:\nBased on the arrangement shown, it appears the soda cans were originally packed in a rectangular grid. A common configuration for large packs of soda cans (such as a 30-pack) is a 6-by-5 grid, totaling 30 cans.\nIf we assume the original arrangement was a full 30-can pack (6 cans across by 5 rows deep):\nThe outer \"border\" layer of such a rectangle consists of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTop row: 6 cans\u003c/li\u003e\n\u003cli\u003eBottom row: 6 cans\u003c/li\u003e\n\u003cli\u003eLeft side (excluding the already counted corners): 3 cans\u003c/li\u003e\n\u003cli\u003eRight side (likewise): 3 cans\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdding these up:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTop: 6\u003c/li\u003e\n\u003cli\u003eBottom: 6\u003c/li\u003e\n\u003cli\u003eLeft (without corners): 3\u003c/li\u003e\n\u003cli\u003eRight (without corners): 3\u003c/li\u003e\n\u003cli\u003eTotal outer ring = 6 + 6 + 3 + 3 = 18 cans\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf only the outer ring of cans remains visible and the entire inner portion is empty, then:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTotal originally: 30 cans\u003c/li\u003e\n\u003cli\u003eCurrently visible (outer ring): 18 cans\u003c/li\u003e\n\u003cli\u003eMissing cans = 30 - 18 = 12\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, there are 12 missing cans.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe soda cans are clearly a 5x7 grid with 4 missing cans. Both models not only get the answer wrong, but they also cannot identify the grid pattern. Furthermore, if you ask them to draw a box around each missing can, they will fail.\u003c/p\u003e\n\u003ch2\u003eOverview\u003c/h2\u003e\n\u003cp\u003eIn this blog post we introduce \u003ca href=\"https://va.landing.ai/agent\"\u003eVisionAgent\u003c/a\u003e, an agentic AI framework that can handle difficult visual tasks that require reasoning. In the first part we will introduce the overall agentic framework for VisionAgent and how it modularizes tool choice to pick the best visual tool for a task. In the second part we will show how we can update the model to use design patterns to solve particularly complex visual problems.\u003c/p\u003e\n\u003ch2\u003eAn Agentic Approach to Visual Reasoning\u003c/h2\u003e\n\u003cp\u003eTo get started, let’s first consider how a human would solve the missing soda can problem. When I solve this task I first locate an empty-looking region in the soda can grid. I then locate cans near the empty region; in this case above and below the regions. To count the number of missing cans I form a sort of grid, so I can see there are 2 cans above the empty spot and therefore the empty spot must consist of 2 missing cans.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/soda_cans.png\" alt=\"soda cans small\"\u003e\u003c/p\u003e\n\u003cp\u003eNow VLMs are not good at localization, so they likely cannot find an empty-looking region like I did. Because they aren’t good at localization, they also cannot find the locations of the soda cans, but there are many tools that would be able to accomplish this task, such as CountGD. So the first task of the agent could be to find a tool that can locate soda cans.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/soda_can_dets.png\" alt=\"soda cans dets\"\u003e\u003c/p\u003e\n\u003cp\u003eGiven that the agent now has the locations of all the soda cans, it can use this information to construct a grid pattern over all the soda cans. Finally, given the soda can detections, and the full grid, it can simply check if each cell contains a soda can detection; if not, it can be counted as a missing can.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003egrid\u003c/th\u003e\n\u003cth align=\"center\"\u003edets\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/grid_image.png\" alt=\"\"\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/all_dets.jpg\" alt=\"\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFrom these steps you can start to see an \u003cstrong\u003eagentic framework\u003c/strong\u003e forming. We can break this down this into 3 main components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eAgentic Tool Selection and Subtasks\u003c/strong\u003e - The overall problem gets divided into solvable subtasks. For instance, in our running example, the first subtask was formulated to detect soda cans, second subtask for constructing the grid pattern and the final subtask for matching detected soda cans to grid cells to find missing soda cans. For each subtask, optimal tool selection has to be performed agentically.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComposing Subtasks\u003c/strong\u003e - We had to compose the subtasks such that the outputs of the previous subtask(s) can be used to solve the new subtasks. For example, to construct the grid we needed the soda can detections from the previous subtask, and to find the missing soda cans we needed the grid and the soda can detections from the previous 2 subtasks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisual Design Patterns\u003c/strong\u003e - Finally, since VLMs struggle at tasks that require visual reasoning, we need to give the agent a hint by telling it about a design pattern it can use to solve this problem. For example, suggesting that it construct a grid and use the grid to detect missing cans. We will cover this in detail in the next blog post.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eImplementing a Visual Reasoning Agent\u003c/h2\u003e\n\u003cp\u003eTo implement this visual reasoning agent, we need to incorporate the three components mentioned above. Before we do that, however, we need a data structure that can handle both inputs to and outputs from these component–allowing the agent to observe and utilize them seamlessly. One way to achieve this is by borrowing the concept of an event stream from \u003ca href=\"https://arxiv.org/abs/2407.16741\"\u003e“Open Hands”\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eEvent Stream Data Structure\u003c/h3\u003e\n\u003cp\u003eAn \u003cstrong\u003eevent stream\u003c/strong\u003e is a structure that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStores\u003c/strong\u003e user messages and agent messages\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDispatches\u003c/strong\u003e actions (like calls to external tools)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCaptures\u003c/strong\u003e the results or observations of those actions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/event_stream.png\" alt=\"event stream\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the diagram mentioned above, note that the box on the left includes \u003cstrong\u003eUSER\u003c/strong\u003e, \u003cstrong\u003eAGENT\u003c/strong\u003e, and \u003cstrong\u003eOBSERVATION\u003c/strong\u003e. Each \u003cstrong\u003eAGENT\u003c/strong\u003e message can contain:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e\u0026#x3C;thinking\u003e\u003c/code\u003e tags for deciding what to do next,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e\u0026#x3C;python\u003e\u003c/code\u003e tags, reflecting the approach from the paper \u003ca href=\"https://arxiv.org/abs/2402.01030\"\u003e“Executable Code Actions Elicit Better LLM Agents\"\u003c/a\u003e. Giving the agent the ability to execute code (rather than just returning text/JSON) has been shown to improve performance. We let it run these Python snippets in a Jupyter notebook–like environment. We call this main agent the planner agent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAgentic Tool Selection and Subtasks\u003c/h3\u003e\n\u003cp\u003eHaving established the \u003cstrong\u003eevent stream\u003c/strong\u003e and basic workflow, we can now discuss how to implement the three core components of our Agentic Framework. Referring back to the soda-can example:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eTool Selection for Detecting Soda Cans\u003c/strong\u003e The first subtask is detecting the soda cans themselves. We delegate this to a specialized subagent—the tool choice agent—which has access to multiple object-detection tools. Its job is to run these tools on the image and pick the one that performs best for our current task. Since we use code as action, we might represent this by a function \u003ccode\u003eget_tool_for_task(image, description)\u003c/code\u003e that returns documentation (or references) for the chosen tool.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/tool_choice.png\" alt=\"tool choice\"\u003e\u003c/p\u003e\n\u003cp\u003eThe agent uses ‘get_tool_for_task’ to compare multiple detectors (OwlV2 and CountGD), automatically picking the one best suited for soda-can detection.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhy Tool Choice Matters\u003c/strong\u003e Selecting the correct tool is challenging, so we handle it in a dedicated tool choice agent. First, the agent interprets the prompt to determine what we’re detecting. It retrieves relevant tools and their documentation, writes code to test each tool on the image, and prints the outputs. A judge agent then compares these outputs to its own approximate prediction.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eNote on the Judge Agent:\u003c/strong\u003e\nThe judge agent doesn’t need to provide an exact count—it just needs to identify which detection result is most plausible based on approximate reasoning. For instance, if the judge guesses 28 cans but sees one tool reporting 5 and another reporting 27, it can rule out 5 as too far off. This allows the system to pick the best available tool without requiring the judge to be perfect. Replacing the judge agent with a human is also an option—sometimes a simple visual check is quicker than labeling everything in detail.\u003c/p\u003e\n\u003ch3\u003eComposing Subtasks\u003c/h3\u003e\n\u003cp\u003eOnce we have the right tool, composing subtasks is straightforward thanks to our “actions as code” approach. The planner agent uses Python to pass outputs from one subtask as inputs to the next. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Step 1: Load the image and pick the best tool for soda-can detection\nimage = load_image(\"image.png\")\nget_tool_for_task(\"Detect all soda cans in the image\", [image])\n\n# Step 2: Use the chosen tool (e.g. CountGD) to perform detection\ndetections = countgd(\"soda can\", image)\n\n# Step 3: Create a grid pattern based on the detections\ngrid = create_grid_pattern(detections)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis modular setup makes it easy to debug each stage. If the detected boxes look incorrect, the planner can try a different confidence threshold or switch to another tool. Because each action is executable Python code, the process of re-running or re-parameterizing is smooth and transparent.\u003c/p\u003e\n\u003ch2\u003eVisual Design Patterns\u003c/h2\u003e\n\u003cp\u003eThe final component is \u003cstrong\u003evisual design patterns\u003c/strong\u003e. We implement this by giving the planner agent a special action called “suggestions,” which serves as a lookup for known design patterns relevant to our problem. The planner agent submits a prompt and image, receiving a 'visual design pattern' suggestion if a suitable one is available.\u003c/p\u003e\n\u003ch3\u003eWhat are Visual Design Patterns?\u003c/h3\u003e\n\u003cp\u003eBefore we dive into the details of this component, it’s important to set some context. If you’ve ever worked on Visual AI projects, you’ll probably know that there are certain visual patterns in images and problems that you can identify and use to draw from a pool of techniques that can be used to solve those problems. Take for example the visual grid pattern and the corresponding design pattern of using an object detection model to construct a grid and identify missing objects. This same pattern actually pops up in many different situations:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eegg carton\u003c/th\u003e\n\u003cth align=\"center\"\u003eparking lot\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/egg_carton.png\" alt=\"\"\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/parking_lot.png\" alt=\"\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003cstrong\u003ehousing development\u003c/strong\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003cstrong\u003eCPU pins\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/houses.png\" alt=\"\"\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003cimg src=\"/assets/blog/vision-agent/pins.jpg\" alt=\"\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSo you can see how having a design pattern to solve one type of problem can be applied to many use cases.\u003c/p\u003e\n\u003ch3\u003eVisual Design Pattern Implementation\u003c/h3\u003e\n\u003cp\u003eTo build the visual design pattern component we have labeled different images and user questions and their corresponding design patterns used to solve them. The component works by taking in the target image and a query, generated from the planner such as “How can I count the number of missing soda cans in this image?” The component then decides if the image and query meet one of the visual patterns we have labeled. If so, it retrieves the design pattern solution, which is a pseudocode solution on how to solve a problem containing that particular visual pattern. This is then returned back to the planner discussed in the previous blog post.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/design_patterns.png\" alt=\"visual design patterns\"\u003e\u003c/p\u003e\n\u003cp\u003eFor example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA user prompt might say, “Identify how many cans are missing in this regular grid.”\u003c/li\u003e\n\u003cli\u003eThe suggestions action checks if any known pattern (like a grid-based arrangement) applies.\u003c/li\u003e\n\u003cli\u003eIf it matches, the action returns a design pattern outline: “Use the detected bounding boxes to infer grid cells, then compare each cell to see which are empty.”\u003c/li\u003e\n\u003cli\u003eThis approach helps the planner agent tackle more complex tasks by following structured strategies—especially when straightforward detection alone isn’t enough.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePutting it All Together\u003c/h2\u003e\n\u003cp\u003eNow that we have covered all 3 components: agentic tool selection and subtasks, composing subtasks and visual design patterns, how do they all work together? We give the planner agent a budget of 10 steps to make when trying to solve a problem, and we also give the planner the ability to use libraries like Matplotlib to plot and visualize results. This allows the planner to go down a path, decide it doesn’t like the results, and then backtrack and find a new path. For example, the planner could use the plan we have described in this blog to find soda cans, but after visualizing the results decide that CountGD did not do a good enough job. It can then backtrack and find a tool to filter the CountGD results such as SigLIP, and then start a new plan where it crops the CountGD objects and filters them with SigLIP to improve performance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/back_tracking.png\" alt=\"back tracking\"\u003e\u003c/p\u003e\n\u003cp\u003eWith these components and agentic workflow in place we can now better reason about vision tasks.\u003c/p\u003e\n\u003cp\u003eThe approach we have taken with VisionAgent can be seen as program synthesis, a topic discussed in Francois Chollet’s \u003ca href=\"https://arxiv.org/abs/1911.01547\"\u003eOn the Measure of Intelligence\u003c/a\u003e paper, where an intelligent system generates a “program” that can be used to solve a task.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/blog/vision-agent/measure_of_intelligence.png\" alt=\"measure of intelligence\"\u003e\u003c/p\u003e\n\u003cp\u003eHere, VisionAgent is the “Intelligent system” and the program it generates for a given task is the “Skill program”. Our tasks are different in the sense that they are vision problems so they do not produce a response or score. However, you can also think of our tasks as the human evaluating the VisionAgent solution, who can provide a response to VisionAgent if it does not perform the task correctly or evaluate it with a score\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this blog post we covered how to use visual design patterns to further enhance our agents visual reasoning skills. We can do all sorts of cool things with this in addition to just finding missing soda cans, such as tracking a queue ball in a pool game, or counting the number of smaller screws on a table. Check out the following prompts and the results from VisionAgent.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Count the number of missing soda cans and draw a bounding box around them.\n\u003cimg src=\"/assets/blog/vision-agent/missing_cans_viz.png\" alt=\"missing cans\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e Count the smaller screws in the image.\n\u003cimg src=\"/assets/blog/vision-agent/screws_viz.png\" alt=\"screws\"\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-5\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl md:text-4xl font-bold tracking-tight md:tracking-tighter leading-tight mb-20 mt-8 flex items-center\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/\",\"className\":\"hover:underline\",\"children\":\"Home\"}],\".\"]}],[\"$\",\"article\",null,{\"className\":\"mb-32\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-5xl md:text-7xl lg:text-8xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center md:text-left\",\"children\":\"VisionAgent: An Agentic Approach for Complex Reasoning\"}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"markdown-styles_markdown__ORfR8\",\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Dillon Laird's Personal Website\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:description\",\"content\":\"Dillon Laird's Personal Website\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:image\",\"content\":\"http://localhost:3000/assets/blog/vision-agent/logo_dark.svg\"}],[\"$\",\"meta\",\"6\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:title\",\"content\":\"VisionAgent: An Agentic Approach for Complex Reasoning | Dillon Laird\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:description\",\"content\":\"Dillon Laird's Personal Website\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:image\",\"content\":\"http://localhost:3000/assets/blog/vision-agent/logo_dark.svg\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script></body></html>